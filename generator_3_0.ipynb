{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generator-3.0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjeuzXw+gdbIOxZvPtGSj1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivendr7/SatGAN/blob/master/generator_3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnK2FWsvZlwu"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.io import read_file\n",
        "from skimage import data, color\n",
        "from skimage.transform import resize\n",
        "url='http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz'\n",
        "path_to_zip=get_file('maps.tar.gz', origin=url, extract=True)\n",
        "inside_train=listdir('/root/.keras/datasets/maps/train')\n",
        "src_list=[]\n",
        "full_path='/root/.keras/datasets/maps/train/'\n",
        "for filename in inside_train:\n",
        "  pixels=load_img(full_path+filename, target_size=(256,512))\n",
        "  pixels=img_to_array(pixels)\n",
        "  src_list.append(pixels[:,:256])\n",
        "src_list=np.asarray(src_list)\n",
        "resized_list=[]\n",
        "for i in src_list:\n",
        "  image=i\n",
        "  image_resized = resize(image, (image.shape[0] // 2, image.shape[1] // 2),\n",
        "                        anti_aliasing=True)\n",
        "  resized_list.append(image_resized)\n",
        "resized_list=np.array(resized_list)\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(src_list[0].astype('uint8'))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(resized_list[0].astype('uint8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y_oRfMXKvBV"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, Concatenate, Lambda, Subtract, Add, Activation, Reshape, Multiply, Flatten, Dropout, Dense, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "import tensorflow as tf\n",
        "from keras.losses import MeanSquaredError \n",
        "import keras.backend as K\n",
        "import numpy as np"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFbBVILjLIn6"
      },
      "source": [
        "def dense_block(in_layer):\n",
        "  init=RandomNormal(stddev=0.2)\n",
        "  conv1=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(in_layer)\n",
        "  conv11=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(conv1)\n",
        "  conv12=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(conv1)\n",
        "  conv13=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(conv1)\n",
        "  concat1=Concatenate()([conv11, conv12, conv13])\n",
        "  conv201=Concatenate()([concat1, conv11])\n",
        "  conv202=Concatenate()([concat1, conv12])\n",
        "  conv203=Concatenate()([concat1, conv13])\n",
        "  conv21i=Conv2D(filters=64, kernel_size=(1,1), padding='same', kernel_initializer=init)(conv201)\n",
        "  conv22i=Conv2D(filters=64, kernel_size=(1,1), padding='same', kernel_initializer=init)(conv202)\n",
        "  conv23i=Conv2D(filters=64, kernel_size=(1,1), padding='same', kernel_initializer=init)(conv203)\n",
        "  conv21=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(conv21i)\n",
        "  conv22=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(conv22i)\n",
        "  conv23=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(conv23i)\n",
        "  concat2=Concatenate()([conv21, conv22, conv23])\n",
        "  conv301=Concatenate()([concat2, conv21i])\n",
        "  conv302=Concatenate()([concat2, conv22i])\n",
        "  conv303=Concatenate()([concat2, conv23i])\n",
        "  conv31i=Conv2D(filters=64, kernel_size=(1,1), padding='same', kernel_initializer=init)(conv301)\n",
        "  conv32i=Conv2D(filters=64, kernel_size=(1,1), padding='same', kernel_initializer=init)(conv302)\n",
        "  conv33i=Conv2D(filters=64, kernel_size=(1,1), padding='same', kernel_initializer=init)(conv303)\n",
        "  conv31=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(conv31i)\n",
        "  conv32=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(conv32i)\n",
        "  conv33=Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer=init)(conv33i)\n",
        "  concat3=Concatenate()([conv31, conv32, conv33])\n",
        "  conv401=Concatenate()([concat3, conv31i])\n",
        "  conv402=Concatenate()([concat3, conv32i])\n",
        "  conv403=Concatenate()([concat3, conv33i])\n",
        "  conv41i=Conv2D(filters=64, kernel_size=(1,1), padding='same', kernel_initializer=init)(conv401)\n",
        "  conv42i=Conv2D(filters=64, kernel_size=(1,1), padding='same', kernel_initializer=init)(conv402)\n",
        "  conv43i=Conv2D(filters=64, kernel_size=(1,1), padding='same', kernel_initializer=init)(conv403)\n",
        "  concat4=Concatenate()([conv41i, conv42i, conv43i])\n",
        "  out_layer=Concatenate()([concat4, conv1])\n",
        "  return out_layer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGR2GVKPQ6H-"
      },
      "source": [
        "inp=Input(shape=(28,28,1))\n",
        "m=Model(inputs=inp, outputs=dense_block(inp))\n",
        "plot_model(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LntY07nDdw_z"
      },
      "source": [
        "def SubpixelConv2D(input_shape, scale=4):\n",
        "    \"\"\"\n",
        "    Keras layer to do subpixel convolution.\n",
        "    NOTE: Tensorflow backend only. Uses tf.depth_to_space\n",
        "    Ref:\n",
        "        [1] Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\n",
        "            Shi et Al.\n",
        "            https://arxiv.org/abs/1609.05158\n",
        "    :param input_shape: tensor shape, (batch, height, width, channel)\n",
        "    :param scale: upsampling scale. Default=4\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # upsample using depth_to_space\n",
        "    def subpixel_shape(input_shape):\n",
        "        dims = [input_shape[0],\n",
        "                input_shape[1] * scale,\n",
        "                input_shape[2] * scale,\n",
        "                int(input_shape[3] / (scale ** 2))]\n",
        "        output_shape = tuple(dims)\n",
        "        return output_shape\n",
        "\n",
        "    def subpixel(x):\n",
        "        return tf.nn.depth_to_space(x, scale)\n",
        "\n",
        "    return Lambda(subpixel, output_shape=subpixel_shape, name='subpixel')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th-EWr55SX2J"
      },
      "source": [
        "def build_udsn(image_shape):\n",
        "  inp=Input(shape=image_shape)\n",
        "  db1=dense_block(inp)\n",
        "  con=Concatenate()([db1, db1])\n",
        "  db2=dense_block(con)\n",
        "  con=Concatenate()([db1, db2, db2])\n",
        "  db3=dense_block(con)\n",
        "  con=Concatenate()([db1, db2, db3, db3])\n",
        "  db4=dense_block(con)\n",
        "  con=Concatenate()([db1, db2, db3, db4, db4])\n",
        "  db5=dense_block(con)\n",
        "  con=Concatenate()([db1, db2, db3, db4, db5, db5])\n",
        "  db6=dense_block(con)\n",
        "  LR=Conv2D(filters=12, kernel_size=(3,3), padding='same')(db6)\n",
        "  LR=LeakyReLU(alpha=0.2)(LR)\n",
        "  Ibase=SubpixelConv2D(LR.shape, scale=2)(LR)\n",
        "  model=Model(inputs=inp, outputs=Ibase)\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsbebfxLZVcl"
      },
      "source": [
        "udsn=build_udsn(image_shape=(128,128,3))\n",
        "plot_model(udsn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWJiCj0iQ5_l"
      },
      "source": [
        "udsn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws4Km7Yt30h_"
      },
      "source": [
        "#eesn essentials\n",
        "def meanfilter(shape, dtype=None):   #kernel to find mean of 3-channel pixel data\n",
        "  f=np.array([\n",
        "              [[[.3333],\n",
        "                [.3333],\n",
        "                [.33333]]]\n",
        "  ])\n",
        "  return K.variable(f, dtype='float32')\n",
        "def laplacian(shape, dtype=None):   #laplacian kernel\n",
        "  f=np.array([\n",
        "      [[[-1]],[[-1]],[[-1]]],\n",
        "      [[[-1]],[[ 8]],[[-1]]],\n",
        "      [[[-1]],[[-1]],[[-1]]]\n",
        "  ])\n",
        "  return K.variable(f, dtype='float32')\n",
        "def channel_thrice(shape, dtype=None):   #kernel to triplicate the single channel data\n",
        "  f=np.array([\n",
        "              [[[1, 1, 1]]]   #3 for three layers of filters\n",
        "  ])\n",
        "  return K.variable(f, dtype='float32')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeWpxGtb2m5c"
      },
      "source": [
        "def build_eesn(Ibase_shape):\n",
        "  inp=Input(shape=(Ibase_shape))\n",
        "  bw=Conv2D(filters=1, kernel_size=(1,1), padding='same', kernel_initializer=meanfilter)(inp)\n",
        "  Iedge=Conv2D(filters=1, kernel_size=(3,3), padding='same', kernel_initializer=laplacian, trainable=False)(bw)     #no change recommended\n",
        "  Itrip=Conv2D(filters=3, kernel_size=(1,1), padding='same', kernel_initializer=channel_thrice, trainable=False)(Iedge)   #no change recommended\n",
        "  \n",
        "  sub=Subtract()([inp, Itrip])\n",
        "  ee=Conv2D(filters=128, kernel_size=(3,3), padding='same', strides=(2,2))(Iedge)\n",
        "  ee=Conv2D(filters=256, kernel_size=(3,3), padding='same')(ee)\n",
        "  ee=Conv2D(filters=64, kernel_size=(1,1), padding='same')(ee)\n",
        "  \n",
        "  #dense net\n",
        "  db=dense_block(ee)\n",
        "  db=dense_block(db)\n",
        "  db=dense_block(db)\n",
        "  db=Conv2D(56, kernel_size=(3,3), padding='same')(db)\n",
        "\n",
        "  #mask branch\n",
        "  mb=Conv2D(filters=64, kernel_size=(3,3), padding='same')(ee)\n",
        "  mb=LeakyReLU(alpha=0.2)(mb)\n",
        "  mb=Conv2D(filters=128, kernel_size=(3,3), padding='same')(mb)\n",
        "  mb=LeakyReLU(alpha=0.2)(mb)\n",
        "  mb=Conv2D(filters=56, kernel_size=(3,3), padding='same')(mb)\n",
        "  mb=LeakyReLU(alpha=0.2)(mb)\n",
        "  mb=Activation('sigmoid')(mb)\n",
        "\n",
        "  ee=Conv2D(4, kernel_size=(3,3), padding='same')(Multiply()([mb, db]))\n",
        "  Istedge=SubpixelConv2D(ee.shape, scale=2)(ee)\n",
        "  #Istedge=Conv2D(1, kernel_size=(3,3), padding='same')(Istedge)\n",
        "  Istedge=Conv2D(filters=3, kernel_size=(1,1), padding='same', kernel_initializer=channel_thrice, trainable=False)(Istedge)  #no change recommended\n",
        "  SR=Add()([Istedge, sub])\n",
        "  model=Model(inputs=inp, outputs=SR)\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v91UoAXTMDZ1"
      },
      "source": [
        "eesn=build_eesn(Ibase_shape=(256,256,3))\n",
        "plot_model(eesn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0duPO9E4TLPR"
      },
      "source": [
        "eesn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjgZwxNRp6qK"
      },
      "source": [
        "def build_discriminator(img_shape):\n",
        "  def block(Input, k, n, s):\n",
        "    l1=Conv2D(n, kernel_size=(k,k), strides=(s,s), padding='same')(Input)\n",
        "    Bn=BatchNormalization()(l1)\n",
        "    Lrlu=LeakyReLU(alpha=0.2)(Bn)\n",
        "    return Lrlu\n",
        "  I1=Input(shape=img_shape)\n",
        "  c1=Conv2D(64, kernel_size=(3,3), padding='same')(I1)\n",
        "  l1=LeakyReLU(alpha=0.2)(c1)\n",
        "  b1=block(l1, 3, 64, 2)\n",
        "  b2=block(b1, 3, 128, 1)\n",
        "  b3=block(b2, 3, 128,2)\n",
        "  b4=block(b3, 3, 256, 1)\n",
        "  b5=block(b4, 3, 256, 2)\n",
        "  b6=block(b5, 3, 512, 1)\n",
        "  b7=block(b6, 3, 512, 2)\n",
        "  b8=Flatten()(b7)\n",
        "  b9=Dropout(.5)(b8)\n",
        "  l8=Dense(1024)(b9)\n",
        "  b10=Dropout(.25)(l8)\n",
        "  l9=LeakyReLU(alpha=0.2)(b10)\n",
        "  l10=Dense(1, activation='sigmoid')(l9)\n",
        "  m=Model(inputs=I1, outputs=l10)\n",
        "  opt=Adam(learning_rate=0.002, beta_1=0.5)\n",
        "  m.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  return m"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkMd7H_lqth_"
      },
      "source": [
        "disc=build_discriminator(img_shape=(256,256,3))\n",
        "disc.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifgh6i8eaZ-M"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx8-QL6Cb0-d"
      },
      "source": [
        "vgg=VGG16()\n",
        "vgg.trainable=False\n",
        "resize_layer=Conv2D(filters=3, kernel_size=(33,33), kernel_initializer=RandomNormal(stddev=0.2), trainable=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AYkinSAb5rJ"
      },
      "source": [
        "def charbonnier_penalty(y_true, y_pred):\n",
        "  epsilon=0.001 #compensation parameter\n",
        "  return K.mean((y_true-y_pred)**2 + epsilon**2)\n",
        "\n",
        "def charbonnier_model(img_shape):\n",
        "  inp=Input(shape=img_shape)\n",
        "  Ibase=udsn(inp)\n",
        "  resize_Ibase=resize_layer(Ibase)\n",
        "  out=vgg(resize_Ibase)\n",
        "  model=Model(inputs=inp, outputs=out)\n",
        "  opt=Adam(learning_rate=0.002, beta_1=0.5)\n",
        "  model.compile(loss=charbonnier_penalty, optimizer=opt, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgzwbHsqclIa"
      },
      "source": [
        "charb=charbonnier_model(img_shape=(128,128,3))\n",
        "charb.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_VR0Q2Dkjp-"
      },
      "source": [
        "def train_charbonnier_model(input_batch, HR_batch):\n",
        "  \n",
        "  #extracting HR Image features through VGG\n",
        "  HR_resized=resize_layer(HR_batch)\n",
        "  ground_feat=vgg(HR_resized)\n",
        "\n",
        "  loss, acc=charb.train_on_batch(input_batch, ground_feat)\n",
        "  return loss, acc"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCM6wMtgl3KZ"
      },
      "source": [
        "def consistency_loss(y_true, y_pred):\n",
        "  lambd=5\n",
        "  return 5*charbonnier_penalty(y_true, y_pred)\n",
        "\n",
        "def consistency_model(img_shape):\n",
        "  inp=Input(shape=img_shape)\n",
        "  Ibase=udsn(inp)\n",
        "  ISR=eesn(Ibase)\n",
        "  model=Model(inputs=inp, outputs=ISR)\n",
        "  opt=Adam(learning_rate=0.002, beta_1=0.5)\n",
        "  model.compile(loss=consistency_loss, optimizer=opt, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-ry7nV-qQMe"
      },
      "source": [
        "cons_model=consistency_model(img_shape=(128,128,3))\n",
        "cons_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb8-pgSXqbty"
      },
      "source": [
        "def train_consistency_model(input_batch, HR_batch):\n",
        "  loss, acc=cons_model.train_on_batch(input_batch, HR_batch)\n",
        "  return loss, acc"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLbTZPWzq2sd"
      },
      "source": [
        "def build_gan(img_shape):\n",
        "  inp=Input(shape=img_shape)\n",
        "  Ibase=udsn(inp)\n",
        "  ISR=eesn(Ibase)\n",
        "  out=disc(ISR)\n",
        "  disc.trainable=False\n",
        "  model=Model(inputs=inp, outputs=out)\n",
        "  opt=Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZdpJ3fCr7lr"
      },
      "source": [
        "gan=build_gan(img_shape=(128,128,3))\n",
        "gan.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY8_CXEdu-kl"
      },
      "source": [
        "def generate_real_samples(n_batch):\n",
        "  ix=np.random.randint(0, 1096, n_batch)\n",
        "  HR_batch=src_list[ix]\n",
        "  y_real=np.ones(shape=(n_batch,))\n",
        "  return HR_batch, y_real\n",
        "\n",
        "def generate_fake_samples(n_batch):\n",
        "  ix=np.random.randint(0, 1096, n_batch)\n",
        "  LR_batch=resized_list[ix]\n",
        "  HR_batch_fake=eesn(udsn(LR_batch))\n",
        "  y_fake=np.zeros(shape=(n_batch,))\n",
        "  return HR_batch_fake, y_fake"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJE1drfJsQGB"
      },
      "source": [
        "def train(n_epochs=20, n_batch=2):\n",
        "  half_batch=n_batch//2\n",
        "  for i in range(n_batch):\n",
        "    ix=np.random.randint(0, 1096, n_batch)\n",
        "    LR_batch=resized_list[ix]\n",
        "    HR_batch=src_list[ix]\n",
        "    \n",
        "    charb_loss, charb_acc=train_charbonnier_model(LR_batch, HR_batch)\n",
        "    cons_loss, cons_acc=train_consistency_model(LR_batch, HR_batch)\n",
        "\n",
        "    real_batch, y_real=generate_real_samples(half_batch)\n",
        "    fake_batch, y_fake=generate_fake_samples(half_batch)\n",
        "\n",
        "    disc_loss_real, disc_acc_real=disc.train_on_batch(real_batch, y_real)\n",
        "    disc_loss_fake, disc_acc_fake=disc.train_on_batch(fake_batch, y_fake)\n",
        "\n",
        "    false_negatives=np.ones(shape=(n_batch,))\n",
        "    gan_loss, gan_acc= gan.train_on_batch(LR_batch, false_negatives)\n",
        "\n",
        "    print('%d charb_loss[%.6f] cons_loss[%.6f] disc_loss_real[%.6f] disc_loss_fake[%.6f] gan_loss[%.6f]'%(i+1, charb_loss, cons_loss, disc_loss_real, \n",
        "                                                                                                          disc_loss_fake, gan_loss))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVJGZEZYyYaj",
        "outputId": "140d4d19-1108-4e60-a811-0ae09f660e69"
      },
      "source": [
        "train(2, 2)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 charb_loss[nan] cons_loss[nan] disc_loss_real[0.000000] disc_loss_fake[nan] gan_loss[nan]\n",
            "2 charb_loss[nan] cons_loss[nan] disc_loss_real[nan] disc_loss_fake[nan] gan_loss[nan]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}